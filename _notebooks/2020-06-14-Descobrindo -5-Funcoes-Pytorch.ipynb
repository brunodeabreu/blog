{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descobrindo 5 funções no Pytorch\n",
    "> \"Recentimente iniciei um curso de Deep Learning pela oferecido pela @freeCodeCamp junto com @JovianML apresentado pelo @aakashns um brilhante instrutor, são seis semanas de curso Zero to GANS. Neste primeiro artigo exploro 5 funções que não conhecia no Pytorch, criando exemplos de aplicações utilizando cada função.\"\n",
    "\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Bruno de Abreu Machado\n",
    "- categories: [pytorch, deep learning]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://miro.medium.com/max/1000/1*FCArwuy8wAglYUXp04SN7g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prevendo Temperatura\n",
    "\n",
    "\n",
    "> Utilizando PyTorch crio uma rede neural simples para tentar realizar a predição de temperatura com dados do oceno, neste notebook/post explico cada etapa do processo de criação de uma rede neural simples com base no conteúdo aprendido no curso  de Deep Learning with PyTorch: Zero to GANs, oferecido pela @freeCodeCamp junto com @JovianML\n",
    "\n",
    "- toc: true \n",
    "- badges: true \n",
    "- comments: true\n",
    "- categories: [pytorch, deep learning]\n",
    "- image: images/chart-preview.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções estudadas neste notebook\n",
    "\n",
    "- torch.chain_matmul\n",
    "- torch.cumprod\n",
    "- torch.det\n",
    "- torch.flatten\n",
    "- torch.lu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como neste notebook vou explorar 5 funções do pytorch nesta sessão de **Imports** foi importado somente a lib `torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "# Import torch and other required modules\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para descobrir novas funções utilizei o comando `dir(<lib>)` que listou todas as funções e o comando `help(torch.<func>)` para conhecer mais sobre ela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temos : 646 funções para explorar\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AVG',\n",
       " 'AggregationType',\n",
       " 'Argument',\n",
       " 'ArgumentSpec',\n",
       " 'Block',\n",
       " 'BoolStorage',\n",
       " 'BoolTensor',\n",
       " 'BoolType',\n",
       " 'ByteStorage',\n",
       " 'ByteTensor',\n",
       " 'CharStorage',\n",
       " 'CharTensor',\n",
       " 'Code',\n",
       " 'CompilationUnit',\n",
       " 'CompleteArgumentSpec',\n",
       " 'CudaBoolStorageBase',\n",
       " 'CudaByteStorageBase',\n",
       " 'CudaCharStorageBase',\n",
       " 'CudaDoubleStorageBase',\n",
       " 'CudaFloatStorageBase',\n",
       " 'CudaHalfStorageBase',\n",
       " 'CudaIntStorageBase',\n",
       " 'CudaLongStorageBase',\n",
       " 'CudaShortStorageBase',\n",
       " 'DictType',\n",
       " 'DoubleStorage',\n",
       " 'DoubleTensor',\n",
       " 'ExecutionPlanState',\n",
       " 'ExtraFilesMap',\n",
       " 'FatalError',\n",
       " 'FileCheck',\n",
       " 'FloatStorage',\n",
       " 'FloatTensor',\n",
       " 'FloatType',\n",
       " 'Function',\n",
       " 'FunctionSchema',\n",
       " 'Future',\n",
       " 'Generator',\n",
       " 'Gradient',\n",
       " 'Graph',\n",
       " 'GraphExecutorState',\n",
       " 'HalfStorage',\n",
       " 'HalfStorageBase',\n",
       " 'HalfTensor',\n",
       " 'IODescriptor',\n",
       " 'IntStorage',\n",
       " 'IntTensor',\n",
       " 'IntType',\n",
       " 'JITException',\n",
       " 'ListType',\n",
       " 'LockingLogger',\n",
       " 'LoggerBase',\n",
       " 'LongStorage',\n",
       " 'LongTensor',\n",
       " 'Node',\n",
       " 'NoopLogger',\n",
       " 'NumberType',\n",
       " 'OptionalType',\n",
       " 'PyTorchFileReader',\n",
       " 'PyTorchFileWriter',\n",
       " 'SUM',\n",
       " 'ScriptMethod',\n",
       " 'ScriptModule',\n",
       " 'ShortStorage',\n",
       " 'ShortTensor',\n",
       " 'Size',\n",
       " 'Storage',\n",
       " 'StringType',\n",
       " 'Tensor',\n",
       " 'TensorType',\n",
       " 'TracingState',\n",
       " 'TupleType',\n",
       " 'Type',\n",
       " 'Use',\n",
       " 'Value',\n",
       " '_C',\n",
       " '_StorageBase',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__config__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_adaptive_avg_pool2d',\n",
       " '_baddbmm_mkl_',\n",
       " '_batch_norm_impl_index',\n",
       " '_cast_Byte',\n",
       " '_cast_Char',\n",
       " '_cast_Double',\n",
       " '_cast_Float',\n",
       " '_cast_Half',\n",
       " '_cast_Int',\n",
       " '_cast_Long',\n",
       " '_cast_Short',\n",
       " '_convolution',\n",
       " '_convolution_nogroup',\n",
       " '_copy_same_type_',\n",
       " '_ctc_loss',\n",
       " '_cudnn_ctc_loss',\n",
       " '_cudnn_init_dropout_state',\n",
       " '_cudnn_rnn',\n",
       " '_cudnn_rnn_flatten_weight',\n",
       " '_cufft_clear_plan_cache',\n",
       " '_cufft_get_plan_cache_max_size',\n",
       " '_cufft_get_plan_cache_size',\n",
       " '_cufft_set_plan_cache_max_size',\n",
       " '_debug_has_internal_overlap',\n",
       " '_dim_arange',\n",
       " '_dirichlet_grad',\n",
       " '_embedding_bag',\n",
       " '_empty_affine_quantized',\n",
       " '_fft_with_size',\n",
       " '_fused_dropout',\n",
       " '_import_dotted_name',\n",
       " '_jit_internal',\n",
       " '_log_softmax',\n",
       " '_log_softmax_backward_data',\n",
       " '_lu_with_info',\n",
       " '_masked_scale',\n",
       " '_mkldnn',\n",
       " '_multinomial_alias_draw',\n",
       " '_multinomial_alias_setup',\n",
       " '_nnpack_available',\n",
       " '_nnpack_spatial_convolution',\n",
       " '_np',\n",
       " '_ops',\n",
       " '_pack_padded_sequence',\n",
       " '_pad_packed_sequence',\n",
       " '_promote_types',\n",
       " '_reshape_from_tensor',\n",
       " '_s_copy_from',\n",
       " '_s_where',\n",
       " '_sample_dirichlet',\n",
       " '_shape_as_tensor',\n",
       " '_six',\n",
       " '_sobol_engine_draw',\n",
       " '_sobol_engine_ff_',\n",
       " '_sobol_engine_initialize_state_',\n",
       " '_sobol_engine_scramble_',\n",
       " '_softmax',\n",
       " '_softmax_backward_data',\n",
       " '_sparse_addmm',\n",
       " '_sparse_mm',\n",
       " '_sparse_sum',\n",
       " '_standard_gamma',\n",
       " '_standard_gamma_grad',\n",
       " '_storage_classes',\n",
       " '_string_classes',\n",
       " '_tensor_classes',\n",
       " '_tensor_str',\n",
       " '_thnn',\n",
       " '_trilinear',\n",
       " '_unique',\n",
       " '_unique2',\n",
       " '_utils',\n",
       " '_utils_internal',\n",
       " '_weight_norm',\n",
       " '_weight_norm_cuda_interface',\n",
       " 'abs',\n",
       " 'abs_',\n",
       " 'acos',\n",
       " 'acos_',\n",
       " 'adaptive_avg_pool1d',\n",
       " 'adaptive_max_pool1d',\n",
       " 'add',\n",
       " 'addbmm',\n",
       " 'addcdiv',\n",
       " 'addcmul',\n",
       " 'addmm',\n",
       " 'addmv',\n",
       " 'addmv_',\n",
       " 'addr',\n",
       " 'affine_grid_generator',\n",
       " 'all',\n",
       " 'allclose',\n",
       " 'alpha_dropout',\n",
       " 'alpha_dropout_',\n",
       " 'any',\n",
       " 'arange',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'argsort',\n",
       " 'as_strided',\n",
       " 'as_strided_',\n",
       " 'as_tensor',\n",
       " 'asin',\n",
       " 'asin_',\n",
       " 'atan',\n",
       " 'atan2',\n",
       " 'atan_',\n",
       " 'autograd',\n",
       " 'avg_pool1d',\n",
       " 'backends',\n",
       " 'baddbmm',\n",
       " 'bartlett_window',\n",
       " 'batch_norm',\n",
       " 'batch_norm_backward_elemt',\n",
       " 'batch_norm_backward_reduce',\n",
       " 'batch_norm_elemt',\n",
       " 'batch_norm_gather_stats',\n",
       " 'batch_norm_stats',\n",
       " 'batch_norm_update_stats',\n",
       " 'bernoulli',\n",
       " 'bilinear',\n",
       " 'binary_cross_entropy_with_logits',\n",
       " 'bincount',\n",
       " 'blackman_window',\n",
       " 'bmm',\n",
       " 'bool',\n",
       " 'broadcast_tensors',\n",
       " 'btrifact',\n",
       " 'btrifact_with_info',\n",
       " 'btrisolve',\n",
       " 'btriunpack',\n",
       " 'cartesian_prod',\n",
       " 'cat',\n",
       " 'cdist',\n",
       " 'ceil',\n",
       " 'ceil_',\n",
       " 'celu',\n",
       " 'celu_',\n",
       " 'chain_matmul',\n",
       " 'cholesky',\n",
       " 'cholesky_inverse',\n",
       " 'cholesky_solve',\n",
       " 'chunk',\n",
       " 'clamp',\n",
       " 'clamp_',\n",
       " 'clamp_max',\n",
       " 'clamp_max_',\n",
       " 'clamp_min',\n",
       " 'clamp_min_',\n",
       " 'clone',\n",
       " 'combinations',\n",
       " 'compiled_with_cxx11_abi',\n",
       " 'complex128',\n",
       " 'complex32',\n",
       " 'complex64',\n",
       " 'constant_pad_nd',\n",
       " 'conv1d',\n",
       " 'conv2d',\n",
       " 'conv3d',\n",
       " 'conv_tbc',\n",
       " 'conv_transpose1d',\n",
       " 'conv_transpose2d',\n",
       " 'conv_transpose3d',\n",
       " 'convolution',\n",
       " 'cos',\n",
       " 'cos_',\n",
       " 'cosh',\n",
       " 'cosh_',\n",
       " 'cosine_embedding_loss',\n",
       " 'cosine_similarity',\n",
       " 'cpp',\n",
       " 'cross',\n",
       " 'ctc_loss',\n",
       " 'cuda',\n",
       " 'cudnn_affine_grid_generator',\n",
       " 'cudnn_batch_norm',\n",
       " 'cudnn_convolution',\n",
       " 'cudnn_convolution_transpose',\n",
       " 'cudnn_grid_sampler',\n",
       " 'cudnn_is_acceptable',\n",
       " 'cumprod',\n",
       " 'cumsum',\n",
       " 'default_generator',\n",
       " 'dequantize',\n",
       " 'det',\n",
       " 'detach',\n",
       " 'detach_',\n",
       " 'device',\n",
       " 'diag',\n",
       " 'diag_embed',\n",
       " 'diagflat',\n",
       " 'diagonal',\n",
       " 'digamma',\n",
       " 'dist',\n",
       " 'distributed',\n",
       " 'distributions',\n",
       " 'div',\n",
       " 'dot',\n",
       " 'double',\n",
       " 'dropout',\n",
       " 'dropout_',\n",
       " 'dsmm',\n",
       " 'dtype',\n",
       " 'eig',\n",
       " 'einsum',\n",
       " 'embedding',\n",
       " 'embedding_bag',\n",
       " 'embedding_renorm_',\n",
       " 'empty',\n",
       " 'empty_like',\n",
       " 'empty_strided',\n",
       " 'enable_grad',\n",
       " 'eq',\n",
       " 'equal',\n",
       " 'erf',\n",
       " 'erf_',\n",
       " 'erfc',\n",
       " 'erfc_',\n",
       " 'erfinv',\n",
       " 'exp',\n",
       " 'exp_',\n",
       " 'expm1',\n",
       " 'expm1_',\n",
       " 'eye',\n",
       " 'fbgemm_is_cpu_supported',\n",
       " 'fbgemm_linear_int8_weight',\n",
       " 'fbgemm_linear_quantize_weight',\n",
       " 'fbgemm_pack_quantized_matrix',\n",
       " 'feature_alpha_dropout',\n",
       " 'feature_alpha_dropout_',\n",
       " 'feature_dropout',\n",
       " 'feature_dropout_',\n",
       " 'fft',\n",
       " 'fill_',\n",
       " 'finfo',\n",
       " 'flatten',\n",
       " 'flip',\n",
       " 'float',\n",
       " 'float16',\n",
       " 'float32',\n",
       " 'float64',\n",
       " 'floor',\n",
       " 'floor_',\n",
       " 'fmod',\n",
       " 'fork',\n",
       " 'frac',\n",
       " 'frac_',\n",
       " 'frobenius_norm',\n",
       " 'from_file',\n",
       " 'from_numpy',\n",
       " 'full',\n",
       " 'full_like',\n",
       " 'functional',\n",
       " 'gather',\n",
       " 'ge',\n",
       " 'gels',\n",
       " 'geqrf',\n",
       " 'ger',\n",
       " 'gesv',\n",
       " 'get_default_dtype',\n",
       " 'get_device',\n",
       " 'get_file_path',\n",
       " 'get_num_threads',\n",
       " 'get_rng_state',\n",
       " 'grid_sampler',\n",
       " 'grid_sampler_2d',\n",
       " 'grid_sampler_3d',\n",
       " 'group_norm',\n",
       " 'gru',\n",
       " 'gru_cell',\n",
       " 'gt',\n",
       " 'half',\n",
       " 'hamming_window',\n",
       " 'hann_window',\n",
       " 'hardshrink',\n",
       " 'has_cuda',\n",
       " 'has_cudnn',\n",
       " 'has_lapack',\n",
       " 'has_mkl',\n",
       " 'has_mkldnn',\n",
       " 'has_openmp',\n",
       " 'hinge_embedding_loss',\n",
       " 'histc',\n",
       " 'hsmm',\n",
       " 'hspmm',\n",
       " 'hub',\n",
       " 'ifft',\n",
       " 'iinfo',\n",
       " 'import_ir_module',\n",
       " 'import_ir_module_from_buffer',\n",
       " 'index_add',\n",
       " 'index_copy',\n",
       " 'index_fill',\n",
       " 'index_put',\n",
       " 'index_put_',\n",
       " 'index_select',\n",
       " 'initial_seed',\n",
       " 'instance_norm',\n",
       " 'int',\n",
       " 'int16',\n",
       " 'int32',\n",
       " 'int64',\n",
       " 'int8',\n",
       " 'int_repr',\n",
       " 'inverse',\n",
       " 'irfft',\n",
       " 'is_anomaly_enabled',\n",
       " 'is_complex',\n",
       " 'is_distributed',\n",
       " 'is_floating_point',\n",
       " 'is_grad_enabled',\n",
       " 'is_nonzero',\n",
       " 'is_same_size',\n",
       " 'is_signed',\n",
       " 'is_storage',\n",
       " 'is_tensor',\n",
       " 'isclose',\n",
       " 'isfinite',\n",
       " 'isinf',\n",
       " 'isnan',\n",
       " 'jit',\n",
       " 'kl_div',\n",
       " 'kthvalue',\n",
       " 'layer_norm',\n",
       " 'layout',\n",
       " 'le',\n",
       " 'lerp',\n",
       " 'lgamma',\n",
       " 'linspace',\n",
       " 'load',\n",
       " 'log',\n",
       " 'log10',\n",
       " 'log10_',\n",
       " 'log1p',\n",
       " 'log1p_',\n",
       " 'log2',\n",
       " 'log2_',\n",
       " 'log_',\n",
       " 'log_softmax',\n",
       " 'logdet',\n",
       " 'logspace',\n",
       " 'logsumexp',\n",
       " 'long',\n",
       " 'lstm',\n",
       " 'lstm_cell',\n",
       " 'lt',\n",
       " 'lu',\n",
       " 'lu_solve',\n",
       " 'lu_unpack',\n",
       " 'manual_seed',\n",
       " 'margin_ranking_loss',\n",
       " 'masked_fill',\n",
       " 'masked_scatter',\n",
       " 'masked_select',\n",
       " 'matmul',\n",
       " 'matrix_power',\n",
       " 'matrix_rank',\n",
       " 'max',\n",
       " 'max_pool1d',\n",
       " 'max_pool1d_with_indices',\n",
       " 'max_pool2d',\n",
       " 'max_pool3d',\n",
       " 'mean',\n",
       " 'median',\n",
       " 'merge_type_from_type_comment',\n",
       " 'meshgrid',\n",
       " 'min',\n",
       " 'miopen_batch_norm',\n",
       " 'miopen_convolution',\n",
       " 'miopen_convolution_transpose',\n",
       " 'miopen_depthwise_convolution',\n",
       " 'mkldnn_convolution',\n",
       " 'mkldnn_convolution_backward_weights',\n",
       " 'mkldnn_max_pool2d',\n",
       " 'mkldnn_reshape',\n",
       " 'mm',\n",
       " 'mode',\n",
       " 'mul',\n",
       " 'multinomial',\n",
       " 'multiprocessing',\n",
       " 'mv',\n",
       " 'mvlgamma',\n",
       " 'name',\n",
       " 'narrow',\n",
       " 'native_batch_norm',\n",
       " 'native_norm',\n",
       " 'ne',\n",
       " 'neg',\n",
       " 'neg_',\n",
       " 'nn',\n",
       " 'no_grad',\n",
       " 'nonzero',\n",
       " 'norm',\n",
       " 'norm_except_dim',\n",
       " 'normal',\n",
       " 'nuclear_norm',\n",
       " 'numel',\n",
       " 'ones',\n",
       " 'ones_like',\n",
       " 'onnx',\n",
       " 'ops',\n",
       " 'optim',\n",
       " 'orgqr',\n",
       " 'ormqr',\n",
       " 'os',\n",
       " 'pairwise_distance',\n",
       " 'parse_ir',\n",
       " 'parse_type_comment',\n",
       " 'pdist',\n",
       " 'pin_memory',\n",
       " 'pinverse',\n",
       " 'pixel_shuffle',\n",
       " 'platform',\n",
       " 'poisson',\n",
       " 'polygamma',\n",
       " 'potrf',\n",
       " 'potri',\n",
       " 'potrs',\n",
       " 'pow',\n",
       " 'prelu',\n",
       " 'prepare_multiprocessing_environment',\n",
       " 'prod',\n",
       " 'pstrf',\n",
       " 'q_scale',\n",
       " 'q_zero_point',\n",
       " 'qint8',\n",
       " 'qr',\n",
       " 'quantize_linear',\n",
       " 'quantized_gru_cell',\n",
       " 'quantized_lstm',\n",
       " 'quantized_lstm_cell',\n",
       " 'quantized_rnn_relu_cell',\n",
       " 'quantized_rnn_tanh_cell',\n",
       " 'quasirandom',\n",
       " 'rand',\n",
       " 'rand_like',\n",
       " 'randint',\n",
       " 'randint_like',\n",
       " 'randn',\n",
       " 'randn_like',\n",
       " 'random',\n",
       " 'randperm',\n",
       " 'range',\n",
       " 'reciprocal',\n",
       " 'reciprocal_',\n",
       " 'relu',\n",
       " 'relu_',\n",
       " 'remainder',\n",
       " 'renorm',\n",
       " 'repeat_interleave',\n",
       " 'reshape',\n",
       " 'resize_as_',\n",
       " 'rfft',\n",
       " 'rnn_relu',\n",
       " 'rnn_relu_cell',\n",
       " 'rnn_tanh',\n",
       " 'rnn_tanh_cell',\n",
       " 'roll',\n",
       " 'rot90',\n",
       " 'round',\n",
       " 'round_',\n",
       " 'rrelu',\n",
       " 'rrelu_',\n",
       " 'rsqrt',\n",
       " 'rsqrt_',\n",
       " 'rsub',\n",
       " 's_copy_',\n",
       " 's_native_addmm',\n",
       " 's_native_addmm_',\n",
       " 'saddmm',\n",
       " 'save',\n",
       " 'scalar_tensor',\n",
       " 'scatter',\n",
       " 'scatter_add',\n",
       " 'select',\n",
       " 'selu',\n",
       " 'selu_',\n",
       " 'serialization',\n",
       " 'set_anomaly_enabled',\n",
       " 'set_default_dtype',\n",
       " 'set_default_tensor_type',\n",
       " 'set_flush_denormal',\n",
       " 'set_grad_enabled',\n",
       " 'set_num_threads',\n",
       " 'set_printoptions',\n",
       " 'set_rng_state',\n",
       " 'short',\n",
       " 'sigmoid',\n",
       " 'sigmoid_',\n",
       " 'sign',\n",
       " 'sin',\n",
       " 'sin_',\n",
       " 'sinh',\n",
       " 'sinh_',\n",
       " 'slogdet',\n",
       " 'smm',\n",
       " 'softmax',\n",
       " 'solve',\n",
       " 'sort',\n",
       " 'sparse',\n",
       " 'sparse_coo',\n",
       " 'sparse_coo_tensor',\n",
       " 'split',\n",
       " 'split_with_sizes',\n",
       " 'spmm',\n",
       " 'sqrt',\n",
       " 'sqrt_',\n",
       " 'squeeze',\n",
       " 'sspaddmm',\n",
       " 'stack',\n",
       " 'std',\n",
       " 'stft',\n",
       " 'storage',\n",
       " 'strided',\n",
       " 'sub',\n",
       " 'sum',\n",
       " 'svd',\n",
       " 'symeig',\n",
       " 'sys',\n",
       " 't',\n",
       " 'take',\n",
       " 'tan',\n",
       " 'tan_',\n",
       " 'tanh',\n",
       " 'tanh_',\n",
       " 'tensor',\n",
       " 'tensordot',\n",
       " 'testing',\n",
       " 'threshold',\n",
       " 'threshold_',\n",
       " 'topk',\n",
       " 'torch',\n",
       " 'trace',\n",
       " 'transpose',\n",
       " 'triangular_solve',\n",
       " 'tril',\n",
       " 'tril_indices',\n",
       " 'triplet_margin_loss',\n",
       " 'triu',\n",
       " 'triu_indices',\n",
       " 'trtrs',\n",
       " 'trunc',\n",
       " 'trunc_',\n",
       " 'typename',\n",
       " 'uint8',\n",
       " 'unbind',\n",
       " 'unique',\n",
       " 'unique_consecutive',\n",
       " 'unsqueeze',\n",
       " 'utils',\n",
       " 'var',\n",
       " 'version',\n",
       " 'wait',\n",
       " 'where',\n",
       " 'zero_',\n",
       " 'zeros',\n",
       " 'zeros_like']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "print('Temos : ' + str(len(dir(torch))) + ' funções para explorar')\n",
    "dir(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lu in module torch.functional:\n",
      "\n",
      "lu(A, pivot=True, get_infos=False, out=None)\n",
      "    Computes the LU factorization of a square matrix or batches of square matrices\n",
      "    :attr:`A`. Returns a tuple containing the LU factorization and pivots of :attr:`A`.\n",
      "    Pivoting is done if :attr:`pivot` is set to ``True``.\n",
      "    \n",
      "    .. note::\n",
      "        The pivots returned by the function are 1-indexed. If :attr:`pivot` is ``False``,\n",
      "        then the returned pivots is a tensor filled with zeros of the appropriate size.\n",
      "    \n",
      "    .. note::\n",
      "        LU factorization with :attr:`pivot` = ``False`` is not available for CPU, and attempting\n",
      "        to do so will throw an error. However, LU factorization with :attr:`pivot` = ``False`` is\n",
      "        available for CUDA.\n",
      "    \n",
      "    .. note::\n",
      "        This function does not check if the factorization was successful or not if\n",
      "        :attr:`get_infos` is ``True`` since the status of the factorization is present in the\n",
      "        third element of the return tuple.\n",
      "    \n",
      "    Arguments:\n",
      "        A (Tensor): the tensor to factor of size :math:`(*, m, m)`\n",
      "        pivot (bool, optional): controls whether pivoting is done. Default: ``True``\n",
      "        get_infos (bool, optional): if set to ``True``, returns an info IntTensor.\n",
      "                                    Default: ``False``\n",
      "        out (tuple, optional): optional output tuple. If :attr:`get_infos` is ``True``,\n",
      "                               then the elements in the tuple are Tensor, IntTensor,\n",
      "                               and IntTensor. If :attr:`get_infos` is ``False``, then the\n",
      "                               elements in the tuple are Tensor, IntTensor. Default: ``None``\n",
      "    \n",
      "    Returns:\n",
      "        (Tensor, IntTensor, IntTensor (optional)): A tuple of tensors containing\n",
      "    \n",
      "            - **factorization** (*Tensor*): the factorization of size :math:`(*, m, m)`\n",
      "    \n",
      "            - **pivots** (*IntTensor*): the pivots of size :math:`(*, m)`\n",
      "    \n",
      "            - **infos** (*IntTensor*, *optional*): if :attr:`get_infos` is ``True``, this is a tensor of\n",
      "              size :math:`(*)` where non-zero values indicate whether factorization for the matrix or\n",
      "              each minibatch has succeeded or failed\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> A = torch.randn(2, 3, 3)\n",
      "        >>> A_LU, pivots = torch.lu(A)\n",
      "        >>> A_LU\n",
      "        tensor([[[ 1.3506,  2.5558, -0.0816],\n",
      "                 [ 0.1684,  1.1551,  0.1940],\n",
      "                 [ 0.1193,  0.6189, -0.5497]],\n",
      "    \n",
      "                [[ 0.4526,  1.2526, -0.3285],\n",
      "                 [-0.7988,  0.7175, -0.9701],\n",
      "                 [ 0.2634, -0.9255, -0.3459]]])\n",
      "        >>> pivots\n",
      "        tensor([[ 3,  3,  3],\n",
      "                [ 3,  3,  3]], dtype=torch.int32)\n",
      "        >>> A_LU, pivots, info = torch.lu(A, get_infos=True)\n",
      "        >>> if info.nonzero().size(0) == 0:\n",
      "        ...   print('LU factorization succeeded for all samples!')\n",
      "        LU factorization succeeded for all samples!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "help(torch.lu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primeira função - torch.chain_matmul\n",
    "\n",
    "Uma das atividades mais utilizadas em *Deep Learning* é a multiplicação de matrizes e uma das funções que o *Pytorch* nos oferece para esta atividade é o `matmul` , onde nos oferece a oportundiade de realizar o produtor de 2 tensores. \n",
    "\n",
    "Nesta primeira função eu destaco a função `chain_matmul` que pode ser utilizada para realizar produtos de N tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criamos 3 matrizes `2x2` utilizando a função `torch.randn` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.0494,  0.4786, -0.2496, -0.9682, -1.1614, -1.6895],\n",
       "         [ 0.4752, -0.0841, -1.4004,  0.0402, -0.5590, -0.9405],\n",
       "         [ 0.5449,  1.5989,  0.5870, -1.2225, -0.3488, -0.2063],\n",
       "         [ 2.5068, -1.1578,  0.4410, -1.4279,  1.5589, -0.1604],\n",
       "         [-0.0883,  1.3295, -1.4044, -0.6259, -1.1656,  0.1526]]),\n",
       " tensor([[ 2.8613,  0.3382,  0.3099,  0.7402],\n",
       "         [-0.6262, -1.3056,  0.6018, -0.6395],\n",
       "         [-0.9182, -0.1888, -0.2580, -0.0872]])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# Example 1 - working \n",
    "\n",
    "matrix1 = torch.randn(3, 4)\n",
    "matrix2 = torch.randn(4, 5)\n",
    "matrix3 = torch.randn(5, 6)\n",
    "\n",
    "# Sample\n",
    "[matrix3, matrix1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar a multiplicação simplesmente passamos as 3 matrizes como argumento.\n",
    "\n",
    "Detalhe sobre as matrizes :\n",
    " * `matrix1` é uma matriz 3x4 : 3 linhas e 4 colunas\n",
    " * `matrix2` é uma matriz 4x5 : 4 linhas e 5 colunas\n",
    " * `matrix3` é uma matriz 5x6 : 5 linhas e 6 colunas\n",
    " \n",
    "O resultado dessa multiplicação é uma nova matriz `3x6`, ou seja, 3 linhsa e 6 colunas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.0324, -2.8931,  0.1645,  5.4147, -1.6135, -0.1331],\n",
       "        [ 0.0715, -1.8514, -0.1220,  0.7759,  1.0616,  1.0326],\n",
       "        [ 3.3314,  0.4199,  0.5218, -2.4496,  1.1681, -0.2269]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# performing product\n",
    "torch.chain_matmul(matrix1, matrix2, matrix3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um segundo exemplo utilizando 4 matrizes maiores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-1.0113,  2.7251,  0.1640,  ..., -1.1982,  0.2516,  2.3802],\n",
       "         [-0.2575,  1.0712, -0.1683,  ...,  0.0799,  0.1335, -0.2284],\n",
       "         [-0.4726,  1.4036, -1.6088,  ...,  0.3482,  0.6061, -0.0266],\n",
       "         ...,\n",
       "         [ 0.7878,  0.5653, -0.7980,  ..., -0.6144,  0.7365, -1.5773],\n",
       "         [-1.2001, -2.0501,  0.2104,  ..., -1.2552,  0.2905, -0.4115],\n",
       "         [ 0.7512,  1.0929, -0.1207,  ..., -2.0084, -0.8357, -1.8306]]),\n",
       " tensor([[-1.0272,  1.3241, -0.7045,  ...,  2.6188,  0.7776,  0.2609],\n",
       "         [-2.2178,  0.2632, -1.0765,  ...,  1.0710, -0.0811, -0.6641],\n",
       "         [ 0.8842, -0.5080,  0.2268,  ...,  0.1498,  0.2623, -1.1105],\n",
       "         ...,\n",
       "         [-1.0267,  0.8322, -1.3487,  ..., -0.7979, -1.6696, -0.4104],\n",
       "         [ 0.9523, -0.4135, -0.2482,  ...,  0.6825, -1.1463, -0.7079],\n",
       "         [ 0.0572,  0.5462,  0.4128,  ...,  1.1507,  1.1007,  2.5236]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# Example 2 - working\n",
    "\n",
    "matrix1 = torch.randn(100,110)\n",
    "matrix2 = torch.randn(110,120)\n",
    "matrix3 = torch.randn(120,130)\n",
    "matrix4 = torch.randn(130,140)\n",
    "\n",
    "\n",
    "\n",
    "# Sample\n",
    "[matrix1, matrix2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste exemplo mostro como a função é eficiente, levando apenas `3.94ms` para realizar a multiplicação de todas as matrizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.14 ms, sys: 802 µs, total: 3.94 ms\n",
      "Wall time: 53.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0312e+03,  2.2024e+03,  8.7111e+02,  ..., -1.6845e+03,\n",
       "         -1.8864e+03,  2.1999e+03],\n",
       "        [-2.6746e+03, -1.2506e+03, -6.2687e+02,  ...,  1.6874e+03,\n",
       "          2.2933e+03, -2.3796e+02],\n",
       "        [ 2.0759e+02, -1.3513e+03, -1.8177e+03,  ..., -9.0427e+02,\n",
       "         -1.5769e+03,  3.6219e+03],\n",
       "        ...,\n",
       "        [-3.3170e+01,  5.5390e+02,  4.1289e+02,  ..., -1.4384e+03,\n",
       "          3.1857e+02,  5.0504e+03],\n",
       "        [ 4.1570e+02, -1.2112e+03,  4.0855e+00,  ..., -3.3088e+02,\n",
       "         -3.1520e+02, -1.3502e+03],\n",
       "        [ 3.5522e+03, -1.2904e+03, -3.7685e+02,  ...,  1.6309e+02,\n",
       "         -3.0861e+02, -3.3388e+02]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# performing product\n",
    "%time torch.chain_matmul(matrix1, matrix2, matrix3,matrix4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No terceiro exemplo destaco um exemplo de falha, tentando realizar multiplicação de matrizes com dimensões diferentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[-1.3402, -1.3130, -0.3395,  0.5945],\n",
       "          [-0.0259, -0.1071, -1.1639,  1.0613],\n",
       "          [-0.7485, -1.1593, -1.5075, -1.0894],\n",
       "          [-1.3146,  2.8806,  0.2349,  0.2622],\n",
       "          [ 1.0905, -0.5099,  1.3923, -0.8981]],\n",
       " \n",
       "         [[-0.7263, -0.2341, -0.5183,  0.3943],\n",
       "          [ 0.6819,  0.1342,  0.0794, -0.0281],\n",
       "          [-1.4046, -1.2519,  0.4766,  0.3968],\n",
       "          [ 0.0881, -0.9358, -1.5493,  0.1794],\n",
       "          [-1.0300,  0.8201, -0.2617,  0.7340]],\n",
       " \n",
       "         [[-0.7806,  1.0247, -0.5675, -0.1108],\n",
       "          [-0.5045, -1.4726,  0.6898, -0.2458],\n",
       "          [ 0.1259, -0.9895,  1.2314,  0.3145],\n",
       "          [ 0.0806,  1.2443, -0.9222,  0.2020],\n",
       "          [-0.6894, -0.1846,  0.5644,  0.1418]],\n",
       " \n",
       "         [[ 0.3160, -0.3909,  0.5069, -2.4691],\n",
       "          [ 0.2330, -1.4668,  0.9580, -0.2209],\n",
       "          [ 1.1199, -0.7593,  0.7848,  0.5991],\n",
       "          [-0.3350, -1.3236, -0.7181,  0.7984],\n",
       "          [-1.2864,  1.1569, -1.7247, -0.4536]]]),\n",
       " tensor([[ 0.5584,  1.1150, -1.2637,  1.5120],\n",
       "         [-0.3575,  1.1148, -1.4004,  2.1986],\n",
       "         [-1.3665,  1.1424,  0.2155, -0.0970]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# Example 3 - breaking \n",
    "\n",
    "matrix1 = torch.randn(3, 4)\n",
    "matrix2 = torch.randn(4, 5, 4)\n",
    "\n",
    "# Sample\n",
    "[matrix2, matrix1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensor dimension is 3, expected 2 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fc1c9bb90b38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#collapse-show\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrix1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatrix2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mchain_matmul\u001b[0;34m(*matrices)\u001b[0m\n\u001b[1;32m    740\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0m_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCLRS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mmitpress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medu\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbooks\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mintroduction\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0malgorithms\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mthird\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0medition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m     \"\"\"\n\u001b[0;32m--> 742\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain_matmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatrices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensor dimension is 3, expected 2 instead."
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "torch.chain_matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que o output de erro do pytorch é bem explicativo, nos retornando que esperava um Tensor de 2D, duas dimensões e recebeu um Tensor 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segunda função - torch.cumprod\n",
    "\n",
    "This second function returns the cumulative product of elements\n",
    "\n",
    "Nesta segunda função gostaria de destacar a função `torch.cumprod` , onde o nome é bem intuitivo quanto sua ação, isto mesmo , esta função realiza produto comulativo de tensores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para exemplificar, neste primeiro exemplo crio um tensor de dois números randomicos e utilizo a função `cumprod` para realizar o produto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9611,  0.5437])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# Example 1 - working\n",
    "\n",
    "nums = torch.randn(2)\n",
    "nums\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9611, -0.5226])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "torch.cumprod(nums,dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser observar o segundo elemento foi substituido pelo produto entre os dois números, nesta função também utilizo o argumento `dim=0`, que indica a dimensão da multiplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.52255007"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "-0.9611 *  0.5437"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste segundo exemplo utilizando ainda a função `cumprod`, vamos realizar o produto cumulativo entre um tensor 3x3 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8956, -1.4917, -0.9016],\n",
       "        [-0.8868,  0.2748, -0.2181],\n",
       "        [ 0.5888,  0.2202, -0.9849]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# Example 2 - working\n",
    "nums = torch.randn(3,3)\n",
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.8956, -2.8277,  2.5494],\n",
       "        [-0.8868, -0.2437,  0.0531],\n",
       "        [ 0.5888,  0.1297, -0.1277]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "torch.cumprod(nums,dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo a multiplicação realizada manualmente, utilizando um tensor `3x3` , utilizando `dim=1` , dimensão 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.549424134432"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "1.8956 * -1.4917 * -0.9016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No terceiro exemplo mostro como se especificar uma dimensão inexistente teremos o seguinte resultado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7083, -1.3484,  2.3374],\n",
       "        [-0.1800, -0.2821,  1.4368],\n",
       "        [ 0.8014, -1.1346, -0.7196]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "# Example 3 - breaking\n",
    "\n",
    "nums = torch.randn(3,3)\n",
    "nums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-f405a7ba577b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcumprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 9)"
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "torch.cumprod(nums,dim=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terceira função - torch.det\n",
    "\n",
    "Esta terceira função é utilizada para calcular a determinante de uma matriz quadrada.\n",
    "\n",
    "Por definição, uma matriz é considerada quadrada quando o número de colunas é igual ao número de linhas e esta associdada a um número que recebe a denominação de determinante .\n",
    "\n",
    "Marcos Noe, no site [brasilescola.uol.com.br](https://brasilescola.uol.com.br/matematica/determinantes-1.htm), explica que :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: O determinante de uma Matriz é dado pelo valor numérico resultante da subtração entre o somatório do produto dos termos da diagonal principal e do somatório do produto dos termos da diagonal secundária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  9.],\n",
       "        [-1.,  6.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "a = torch.tensor([[2,9],[-1,6.]])\n",
    "a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste primeiro exemplo tempos :\n",
    "* A diagonal principal 2,6, onde o produto é  : `2 * 6 = 12`\n",
    "* A diagonal secundária -1,9 , o produto será :  `-1 * 9 = -9 `\n",
    "* E por fim a soma : `12 - (-9) = 21`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(21.)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste segundo exemplo vamos ver uma matriz `3x3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1348, -1.0434,  0.0589],\n",
       "        [ 0.2338,  1.0583, -0.1160],\n",
       "        [-0.6723,  0.7773,  0.8919]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "\n",
    "\n",
    "A = torch.randn(3, 3)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0493)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pytorch trabalha com valores em *float* não podemos apresentar valores inteiros para a função pois como documentado ela espera um valor do time `float`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  9],\n",
       "        [-1,  6]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3 - breaking \n",
    "\n",
    "b = torch.tensor([[2,9],[-1,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a floating point tensor as input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-eaa77d970a4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected a floating point tensor as input"
     ]
    }
   ],
   "source": [
    "torch.det(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quarta função - torch.flatten\n",
    "\n",
    "Esta função é utilizada para transformar uma matriz em um vetor, ou seja, como demostrado no exemplo abaixo ela pega uma matriz 2x2 e transforma em um vetor unidimensional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "T = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver no segundo exemplo temos um tensor 2x2x2 , ou seja duas matrizes, neste caso ao realizar o `flatten` teremos o seguinte output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [3, 4]],\n",
       "\n",
       "        [[5, 6],\n",
       "         [7, 8]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - working\n",
    "\n",
    "\n",
    "T = torch.tensor([[[1, 2],\n",
    "                   [3, 4]],\n",
    "                  [[5, 6],\n",
    "                   [7, 8]]])\n",
    "\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3, 4],\n",
       "        [5, 6, 7, 8]])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(T, start_dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar como no exemplo o parametro `start_dim` para espeficicar a primera dimensão.\n",
    "\n",
    "Abaixo teremos um exemplo onde não se pode aplicar o flatten quando range de dimensões não é válido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3 - breaking \n",
    "\n",
    "\n",
    "T = torch.tensor([[1, 2],\n",
    "                  [3, 4]])\n",
    "\n",
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-165-f58365975ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "torch.flatten(T, start_dim=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quinta função  - torch.lu\n",
    "\n",
    "A última função que destaco, mas não menos importante é a `torch.lu` , esta interessante função retorna uma tupla contendo **LU factorization** e **pivots**, no primeiro exemplo criamos um tensor 2x3x3 e aplicamos a função:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8440,  0.3518, -0.0991],\n",
       "         [ 0.6065, -1.2822, -1.2502],\n",
       "         [-0.9766,  0.5978, -0.9145]],\n",
       "\n",
       "        [[-0.4165,  1.0368, -0.7703],\n",
       "         [ 0.3856, -0.8209, -0.9273],\n",
       "         [-0.9241, -0.7736, -0.5355]]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - working\n",
    "\n",
    "A = torch.randn(2, 3, 3)\n",
    "A_LU, pivots = torch.lu(A, pivot=True)\n",
    "A_LU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3, 3],\n",
       "        [2, 2, 3]], dtype=torch.int32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LU factorization succeeded for all samples!\n"
     ]
    }
   ],
   "source": [
    " A_LU, pivots, info = torch.lu(A, get_infos=True)\n",
    "if info.nonzero().size(0) == 0:\n",
    "    print('LU factorization succeeded for all samples!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todo exemplo quero mostrar uma utilização com erro, como podemos ver algumas opção só estão disponiveis para processamento em `GPU/CUDA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "lu without pivoting is not implemented on the CPU",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-c3dc98d6d278>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mA_LU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mA_LU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m_lu_no_infos\u001b[0;34m(A, pivot, get_infos, out)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             return handle_torch_function(\n\u001b[1;32m   1028\u001b[0m                 lu, (A,), A, pivot=pivot, get_infos=get_infos, out=out)\n\u001b[0;32m-> 1029\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lu_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0m_check_list_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36m_lu_impl\u001b[0;34m(A, pivot, get_infos, out)\u001b[0m\n\u001b[1;32m    993\u001b[0m     \"\"\"\n\u001b[1;32m    994\u001b[0m     \u001b[0;31m# If get_infos is True, then we don't need to check for errors and vice versa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lu_with_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpivot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_list_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: lu without pivoting is not implemented on the CPU"
     ]
    }
   ],
   "source": [
    "# Example 3 - breaking \n",
    "\n",
    "A = torch.randn(2, 3, 3)\n",
    "A_LU, pivots = torch.lu(A, pivot=False)\n",
    "A_LU\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Aprendi muito ao pesquisar as funções da lib torch, minha jornada em Deep Learning está no começo mas é muito estimulante ver como é facil a utilização e documentação, apesar do notebook ser uma lista de 5 funções tive o prazer de estudar diversas outras como `torch.randn`.\n",
    "\n",
    "Este notebook é a parte da primeira aula do curso [Zero to Gans](https://jovian.ml/forum/c/pytorch-zero-to-gans) oferecido pela @Jovian.ml e @freecodecamp, muito obrigado @aakashns pela brilhante aula.\n",
    "\n",
    "Thank you @aakashns for the fantastic course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Links\n",
    "Algumas referencias :\n",
    "\n",
    "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video aula 1 - Zero To Gans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> youtube: https://youtu.be/vo_fUOk-IKk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
